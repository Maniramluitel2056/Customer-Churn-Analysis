# Predictive Modeling with ANN and Tallying Predictions for Customer Churn Analysis
# Objective:
The objective of this task was to define, implement, and evaluate an Artificial Neural Network (ANN) model to predict customer churn. Additionally, the results from the model were tallied with actual test data to validate the predictions.

# Steps Taken:
1. Setting Up the Configuration:
    - Objective: Ensure that all file paths, directories, and configurations were centrally managed using a `config.json` file located in the root directory.
    - Explanation:
      - The `config.json` file contains paths to datasets, models, and results.
      - Paths were dynamically loaded in the `predictive_modeling.py` and `PredictionTally.py` scripts to ensure consistency and flexibility.
      
2. Loading the Configuration:
    - Objective: Load the configuration file to obtain paths and settings needed for the predictive modeling and tally tasks.
    - Explanation:
      - In both `predictive_modeling.py` and `PredictionTally.py`, the configuration file was loaded using Python's `json` module.
      - The `os` module was used to construct absolute paths based on the project's root directory.
      
3. Defining and Training the ANN Model:
    - Objective: Design and train an ANN model using the train and test datasets.
    - Explanation:
      - The ANN model was defined using TensorFlow's Keras API with layers appropriate for binary classification (customer churn).
      - The model was trained using the provided training dataset and validated against the test dataset to ensure generalization.
      
4. Saving Model and Results:
    - Objective: Save the trained model, model architecture, and training results for future reference and use.
    - Explanation:
      - The best model during training was saved using `ModelCheckpoint`.
      - The model architecture was saved in a JSON format.
      - The training history (e.g., accuracy and loss per epoch) was saved in a text file.
      - Predictions were generated and saved in a CSV file for further analysis.
      
5. Running the Prediction Tally:
    - Objective: Tally the predictions generated by the ANN model with the actual values from the test dataset.
    - Explanation:
      - The `PredictionTally.py` script was used to compare the predicted results with the actual labels in the test dataset.
      - The tally results, including any mismatches, were saved in a CSV file for analysis.
      
6. Integration with Utility Functions:
    - Objective: Modularize the code by moving the tally functionality to a utility function for better organization and reusability.
    - Explanation:
      - The `run_prediction_tally` function was moved to `PredictionTally.py` under the `utils` directory.
      - This function was imported and executed within `predictive_modeling.py`, ensuring that predictions were tallied every time the model was run.
      
# Results Obtained:
- Model Architecture: The architecture of the ANN was saved as `ann_architecture.json`.
- Trained Model: The best model was saved as `best_model.h5`.
- Training History: The training accuracy and loss over epochs were saved as `training_results.txt`.
- Predictions: The model's predictions were saved as `predictions.csv`.
- Tally Results: The comparison between predictions and actual values, including any mismatches, were saved as `tally_results.csv`.

# Summary:
This task involved designing and implementing an ANN model to predict customer churn. The model was trained and validated using train and test datasets, with the architecture, best model, and training history being saved for further analysis. Predictions were generated, saved, and tallied against the actual test data to ensure the model's accuracy.

# Next Steps:
- Refinement of the Model: Based on the tally results, consider refining the model to improve accuracy, especially if any mismatches were found.
- Further Evaluation: Use additional metrics or techniques to evaluate the model's performance more comprehensively.
- Deployment: Depending on the model's performance, consider deploying it for production use or in a more extensive analysis pipeline.