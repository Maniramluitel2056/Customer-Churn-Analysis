{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining ANN Architecture\n",
    "\n",
    "In this notebook, we will define the architecture of the Artificial Neural Network (ANN) model as part of the predictive modeling process. This task involves setting up the input, hidden, and output layers, selecting the appropriate activation functions, and defining the optimization algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Environment and Seeding\n",
    "\n",
    "To ensure reproducibility and consistent results every time we run the notebook, we set a seed value. This seed value helps in generating the same sequence of random numbers across different runs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pandas as pd\n",
    "import importlib.util\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Dropout # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore # type: ignore\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint # type: ignore\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Ensure that the 'utils' directory is correctly added to the Python path\n",
    "utils_path = os.path.abspath('../../utils')\n",
    "sys.path.append(utils_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Configuration\n",
    "\n",
    "In this step, we load the configuration file that contains the paths to the datasets and directories needed for the predictive modeling task. The `config.json` file, located in the root directory, is accessed and loaded into the environment to provide the necessary paths for the data processing steps.\n",
    "\n",
    "This configuration file is crucial as it allows us to dynamically change paths without hardcoding them, making the code more flexible and easier to manage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_data_path': 'data/raw/Dataset (ATS)-1.csv',\n",
       " 'interim_cleaned_data_path': 'data/interim/cleaned_dataset.csv',\n",
       " 'preprocessed_data_path': 'Data_Preparation/preprocessed_dataset/cleaned_dataset.csv',\n",
       " 'processed_data_path': 'data/processed/processed_dataset_with_features.csv',\n",
       " 'train_data_path': 'data/train/train_dataset.csv',\n",
       " 'test_data_path': 'data/test/test_dataset.csv',\n",
       " 'min_max_scaled_path': 'Data_Preparation/scaling_techniques/min_max_scaled_dataset.csv',\n",
       " 'standard_scaled_path': 'Data_Preparation/scaling_techniques/standard_scaled_dataset.csv',\n",
       " 'training_set_path': 'Data_Preparation/training_sets/train_dataset.csv',\n",
       " 'testing_set_path': 'Data_Preparation/testing_sets/test_dataset.csv',\n",
       " 'min-max_scaled_4_clusters_path': 'Clustering_Analysis/kmeans_model/min-max_scaled_4_clusters.csv',\n",
       " 'standard_scaled_4_clusters_path': 'Clustering_Analysis/kmeans_model/standard_scaled_4_clusters.csv',\n",
       " 'min-max_scaled_cluster_characteristics_path': 'Clustering_Analysis/kmeans_model/min-max_scaled_cluster_characteristics.csv',\n",
       " 'standard_scaled_cluster_characteristics_path': 'Clustering_Analysis/kmeans_model/standard_scaled_cluster_characteristics.csv',\n",
       " 'trained_model_path': 'Predictive_Modeling/trained_model',\n",
       " 'ann_architecture_path': 'Predictive_Modeling/ann_architecture',\n",
       " 'results_path': 'Predictive_Modeling/results',\n",
       " 'cluster_train_path': 'Clustering_Analysis/kmeans_model/min-max_scaled_4_clusters.csv',\n",
       " 'cluster_test_path': 'Clustering_Analysis/kmeans_model/standard_scaled_4_clusters.csv',\n",
       " 'predictions_path': 'Predictive_Modeling/results/predictions.csv',\n",
       " 'training_results_path': 'Predictive_Modeling/results/training_results.csv'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the base directory (the root of your project)\n",
    "base_dir = os.path.dirname(os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "# Load the configuration file from the main directory\n",
    "config_path = os.path.join(base_dir, 'config.json')\n",
    "with open(config_path) as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Display the loaded configuration for verification\n",
    "config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Next, we load the training and testing datasets as specified in the configuration file. The training dataset will be used to train the model, and the testing dataset will be used to evaluate the model's performance.\n",
    "\n",
    "It's important to ensure that the data is loaded correctly and matches the structure expected by the model, which includes features (X) and the target variable (y) for both training and testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define paths using absolute paths\n",
    "train_data_path = os.path.join(base_dir, config['train_data_path'])\n",
    "test_data_path = os.path.join(base_dir, config['test_data_path'])\n",
    "trained_model_path = os.path.join(base_dir, config['trained_model_path'])\n",
    "ann_architecture_path = os.path.join(base_dir, config['ann_architecture_path'])\n",
    "results_path = os.path.join(base_dir, config['results_path'])\n",
    "\n",
    "# Load the training and testing data\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# Separate features (all columns except the target) and target (the target column) for both training and testing data\n",
    "X_train = train_data.drop(columns=['Churn_No', 'Churn_Yes'])  # Drop the target columns to get features\n",
    "y_train = train_data['Churn_Yes']  # Select the target column\n",
    "\n",
    "X_test = test_data.drop(columns=['Churn_No', 'Churn_Yes'])\n",
    "y_test = test_data['Churn_Yes']  # Select the target column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the ANN Architecture\n",
    "\n",
    "In this step, we define the architecture of the Artificial Neural Network (ANN). The model structure includes:\n",
    "\n",
    "- **Input Layer**: Accepts input features.\n",
    "- **Hidden Layers**: Performs computations and learning.\n",
    "- **Output Layer**: Provides the final prediction.\n",
    "\n",
    "We also specify the activation functions (`relu` for hidden layers and `sigmoid` for the output layer) and the optimizer (`Adam`) to use during training.\n",
    "\n",
    "The architecture is crucial for determining the model's ability to learn patterns in the data and make accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the architecture of the ANN model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(units=64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping Callback\n",
    "\n",
    "To prevent overfitting and ensure that our model stops training when it no longer improves, we define an **Early Stopping** callback. This callback monitors the validation loss during training and stops the training process if the loss doesn't improve for a specified number of epochs. We also enable the option to restore the model's best weights after stopping.\n",
    "\n",
    "The parameters we used for Early Stopping are:\n",
    "- **Monitor**: `val_loss` - We monitor the validation loss to decide when to stop.\n",
    "- **Patience**: `10` - The training will stop if the validation loss doesn't improve after 10 epochs.\n",
    "- **Restore Best Weights**: `True` - After stopping, the model's weights will revert to those of the epoch with the best validation loss.\n",
    "\n",
    "This strategy helps to avoid overfitting by stopping the training at the optimal point where the model performs best on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model and Optimize Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Validation\n",
    "\n",
    "In this step, we train the ANN model using the training data. During training, the model learns the relationships between the input features and the target variable. We use a validation set (testing data) to monitor the model's performance and adjust the training process accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "168/177 [===========================>..] - ETA: 0s - loss: 13.5138 - accuracy: 0.6432\n",
      "Epoch 1: val_loss improved from inf to 0.79533, saving model to d:\\Customer-Churn-Analysis\\Predictive_Modeling/trained_model\\best_model.h5\n",
      "177/177 [==============================] - 2s 4ms/step - loss: 12.9911 - accuracy: 0.6424 - val_loss: 0.7953 - val_accuracy: 0.7764\n",
      "Epoch 2/50\n",
      " 58/177 [========>.....................] - ETA: 0s - loss: 1.1428 - accuracy: 0.7074"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mujta\\anaconda3\\envs\\churn_analysis\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/177 [===========================>..] - ETA: 0s - loss: 0.9420 - accuracy: 0.7197\n",
      "Epoch 2: val_loss improved from 0.79533 to 0.54831, saving model to d:\\Customer-Churn-Analysis\\Predictive_Modeling/trained_model\\best_model.h5\n",
      "177/177 [==============================] - 0s 3ms/step - loss: 0.9319 - accuracy: 0.7208 - val_loss: 0.5483 - val_accuracy: 0.7786\n",
      "Epoch 3/50\n",
      "148/177 [========================>.....] - ETA: 0s - loss: 0.6482 - accuracy: 0.7249\n",
      "Epoch 3: val_loss improved from 0.54831 to 0.53577, saving model to d:\\Customer-Churn-Analysis\\Predictive_Modeling/trained_model\\best_model.h5\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.6471 - accuracy: 0.7263 - val_loss: 0.5358 - val_accuracy: 0.7828\n",
      "Epoch 4/50\n",
      "151/177 [========================>.....] - ETA: 0s - loss: 0.5729 - accuracy: 0.7517\n",
      "Epoch 4: val_loss did not improve from 0.53577\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5745 - accuracy: 0.7515 - val_loss: 0.5653 - val_accuracy: 0.7779\n",
      "Epoch 5/50\n",
      "156/177 [=========================>....] - ETA: 0s - loss: 0.5326 - accuracy: 0.7694\n",
      "Epoch 5: val_loss improved from 0.53577 to 0.50427, saving model to d:\\Customer-Churn-Analysis\\Predictive_Modeling/trained_model\\best_model.h5\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5314 - accuracy: 0.7687 - val_loss: 0.5043 - val_accuracy: 0.7693\n",
      "Epoch 6/50\n",
      "169/177 [===========================>..] - ETA: 0s - loss: 0.5287 - accuracy: 0.7618\n",
      "Epoch 6: val_loss improved from 0.50427 to 0.50242, saving model to d:\\Customer-Churn-Analysis\\Predictive_Modeling/trained_model\\best_model.h5\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5276 - accuracy: 0.7623 - val_loss: 0.5024 - val_accuracy: 0.7665\n",
      "Epoch 7/50\n",
      "152/177 [========================>.....] - ETA: 0s - loss: 0.5285 - accuracy: 0.7619\n",
      "Epoch 7: val_loss did not improve from 0.50242\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5287 - accuracy: 0.7630 - val_loss: 0.5086 - val_accuracy: 0.7842\n",
      "Epoch 8/50\n",
      "170/177 [===========================>..] - ETA: 0s - loss: 0.5492 - accuracy: 0.7640\n",
      "Epoch 8: val_loss did not improve from 0.50242\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5489 - accuracy: 0.7638 - val_loss: 0.5269 - val_accuracy: 0.7658\n",
      "Epoch 9/50\n",
      "163/177 [==========================>...] - ETA: 0s - loss: 0.5317 - accuracy: 0.7613\n",
      "Epoch 9: val_loss did not improve from 0.50242\n",
      "177/177 [==============================] - 0s 3ms/step - loss: 0.5312 - accuracy: 0.7611 - val_loss: 0.5038 - val_accuracy: 0.7828\n",
      "Epoch 10/50\n",
      "167/177 [===========================>..] - ETA: 0s - loss: 0.5333 - accuracy: 0.7625\n",
      "Epoch 10: val_loss did not improve from 0.50242\n",
      "177/177 [==============================] - 1s 3ms/step - loss: 0.5350 - accuracy: 0.7613 - val_loss: 0.5184 - val_accuracy: 0.7729\n",
      "Epoch 11/50\n",
      "170/177 [===========================>..] - ETA: 0s - loss: 0.5328 - accuracy: 0.7607\n",
      "Epoch 11: val_loss did not improve from 0.50242\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5307 - accuracy: 0.7622 - val_loss: 0.5125 - val_accuracy: 0.7722\n",
      "Epoch 12/50\n",
      "150/177 [========================>.....] - ETA: 0s - loss: 0.5229 - accuracy: 0.7683\n",
      "Epoch 12: val_loss did not improve from 0.50242\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5255 - accuracy: 0.7655 - val_loss: 0.5036 - val_accuracy: 0.7736\n",
      "Epoch 13/50\n",
      "155/177 [=========================>....] - ETA: 0s - loss: 0.5174 - accuracy: 0.7714\n",
      "Epoch 13: val_loss improved from 0.50242 to 0.48428, saving model to d:\\Customer-Churn-Analysis\\Predictive_Modeling/trained_model\\best_model.h5\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5190 - accuracy: 0.7698 - val_loss: 0.4843 - val_accuracy: 0.7786\n",
      "Epoch 14/50\n",
      "163/177 [==========================>...] - ETA: 0s - loss: 0.5239 - accuracy: 0.7653\n",
      "Epoch 14: val_loss did not improve from 0.48428\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5243 - accuracy: 0.7650 - val_loss: 0.4971 - val_accuracy: 0.7757\n",
      "Epoch 15/50\n",
      "175/177 [============================>.] - ETA: 0s - loss: 0.5263 - accuracy: 0.7670\n",
      "Epoch 15: val_loss did not improve from 0.48428\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5257 - accuracy: 0.7675 - val_loss: 0.4930 - val_accuracy: 0.7821\n",
      "Epoch 16/50\n",
      "146/177 [=======================>......] - ETA: 0s - loss: 0.5102 - accuracy: 0.7789\n",
      "Epoch 16: val_loss did not improve from 0.48428\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5175 - accuracy: 0.7726 - val_loss: 0.4929 - val_accuracy: 0.7750\n",
      "Epoch 17/50\n",
      "151/177 [========================>.....] - ETA: 0s - loss: 0.5136 - accuracy: 0.7684\n",
      "Epoch 17: val_loss improved from 0.48428 to 0.46717, saving model to d:\\Customer-Churn-Analysis\\Predictive_Modeling/trained_model\\best_model.h5\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5163 - accuracy: 0.7641 - val_loss: 0.4672 - val_accuracy: 0.7984\n",
      "Epoch 18/50\n",
      "155/177 [=========================>....] - ETA: 0s - loss: 0.5083 - accuracy: 0.7740\n",
      "Epoch 18: val_loss did not improve from 0.46717\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5079 - accuracy: 0.7751 - val_loss: 0.4940 - val_accuracy: 0.7786\n",
      "Epoch 19/50\n",
      "175/177 [============================>.] - ETA: 0s - loss: 0.5102 - accuracy: 0.7659\n",
      "Epoch 19: val_loss did not improve from 0.46717\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7666 - val_loss: 0.4855 - val_accuracy: 0.7793\n",
      "Epoch 20/50\n",
      "173/177 [============================>.] - ETA: 0s - loss: 0.5080 - accuracy: 0.7760\n",
      "Epoch 20: val_loss did not improve from 0.46717\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5077 - accuracy: 0.7764 - val_loss: 0.4828 - val_accuracy: 0.7786\n",
      "Epoch 21/50\n",
      "169/177 [===========================>..] - ETA: 0s - loss: 0.5128 - accuracy: 0.7740\n",
      "Epoch 21: val_loss did not improve from 0.46717\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7744 - val_loss: 0.5037 - val_accuracy: 0.7835\n",
      "Epoch 22/50\n",
      "175/177 [============================>.] - ETA: 0s - loss: 0.5146 - accuracy: 0.7739\n",
      "Epoch 22: val_loss did not improve from 0.46717\n",
      "177/177 [==============================] - 1s 3ms/step - loss: 0.5139 - accuracy: 0.7744 - val_loss: 0.4770 - val_accuracy: 0.7828\n",
      "Epoch 23/50\n",
      "156/177 [=========================>....] - ETA: 0s - loss: 0.5202 - accuracy: 0.7654\n",
      "Epoch 23: val_loss did not improve from 0.46717\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5247 - accuracy: 0.7625 - val_loss: 0.4856 - val_accuracy: 0.7864\n",
      "Epoch 24/50\n",
      "174/177 [============================>.] - ETA: 0s - loss: 0.5142 - accuracy: 0.7730\n",
      "Epoch 24: val_loss did not improve from 0.46717\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5142 - accuracy: 0.7730 - val_loss: 0.4820 - val_accuracy: 0.7857\n",
      "Epoch 25/50\n",
      "166/177 [===========================>..] - ETA: 0s - loss: 0.5254 - accuracy: 0.7694\n",
      "Epoch 25: val_loss did not improve from 0.46717\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5215 - accuracy: 0.7721 - val_loss: 0.4819 - val_accuracy: 0.7835\n",
      "Epoch 26/50\n",
      "161/177 [==========================>...] - ETA: 0s - loss: 0.5101 - accuracy: 0.7708\n",
      "Epoch 26: val_loss did not improve from 0.46717\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7675 - val_loss: 0.4975 - val_accuracy: 0.7828\n",
      "Epoch 27/50\n",
      "151/177 [========================>.....] - ETA: 0s - loss: 0.5066 - accuracy: 0.7713\n",
      "Epoch 27: val_loss improved from 0.46717 to 0.46433, saving model to d:\\Customer-Churn-Analysis\\Predictive_Modeling/trained_model\\best_model.h5\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5080 - accuracy: 0.7707 - val_loss: 0.4643 - val_accuracy: 0.7949\n",
      "Epoch 28/50\n",
      "157/177 [=========================>....] - ETA: 0s - loss: 0.5099 - accuracy: 0.7713\n",
      "Epoch 28: val_loss improved from 0.46433 to 0.45712, saving model to d:\\Customer-Churn-Analysis\\Predictive_Modeling/trained_model\\best_model.h5\n",
      "177/177 [==============================] - 1s 3ms/step - loss: 0.5098 - accuracy: 0.7707 - val_loss: 0.4571 - val_accuracy: 0.8041\n",
      "Epoch 29/50\n",
      "161/177 [==========================>...] - ETA: 0s - loss: 0.5062 - accuracy: 0.7741\n",
      "Epoch 29: val_loss did not improve from 0.45712\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.7744 - val_loss: 0.5771 - val_accuracy: 0.6678\n",
      "Epoch 30/50\n",
      "174/177 [============================>.] - ETA: 0s - loss: 0.5168 - accuracy: 0.7674\n",
      "Epoch 30: val_loss did not improve from 0.45712\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5163 - accuracy: 0.7678 - val_loss: 0.4673 - val_accuracy: 0.7956\n",
      "Epoch 31/50\n",
      "175/177 [============================>.] - ETA: 0s - loss: 0.4985 - accuracy: 0.7757\n",
      "Epoch 31: val_loss did not improve from 0.45712\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.4986 - accuracy: 0.7755 - val_loss: 0.4830 - val_accuracy: 0.7857\n",
      "Epoch 32/50\n",
      "174/177 [============================>.] - ETA: 0s - loss: 0.5001 - accuracy: 0.7739\n",
      "Epoch 32: val_loss did not improve from 0.45712\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.4991 - accuracy: 0.7749 - val_loss: 0.4796 - val_accuracy: 0.7835\n",
      "Epoch 33/50\n",
      "150/177 [========================>.....] - ETA: 0s - loss: 0.4981 - accuracy: 0.7792\n",
      "Epoch 33: val_loss improved from 0.45712 to 0.45604, saving model to d:\\Customer-Churn-Analysis\\Predictive_Modeling/trained_model\\best_model.h5\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.4985 - accuracy: 0.7799 - val_loss: 0.4560 - val_accuracy: 0.8098\n",
      "Epoch 34/50\n",
      "165/177 [==========================>...] - ETA: 0s - loss: 0.5056 - accuracy: 0.7720\n",
      "Epoch 34: val_loss did not improve from 0.45604\n",
      "177/177 [==============================] - 1s 3ms/step - loss: 0.5080 - accuracy: 0.7710 - val_loss: 0.5000 - val_accuracy: 0.7878\n",
      "Epoch 35/50\n",
      "166/177 [===========================>..] - ETA: 0s - loss: 0.5047 - accuracy: 0.7750\n",
      "Epoch 35: val_loss did not improve from 0.45604\n",
      "177/177 [==============================] - 1s 3ms/step - loss: 0.5053 - accuracy: 0.7741 - val_loss: 0.4661 - val_accuracy: 0.7942\n",
      "Epoch 36/50\n",
      "154/177 [=========================>....] - ETA: 0s - loss: 0.5071 - accuracy: 0.7693\n",
      "Epoch 36: val_loss did not improve from 0.45604\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5040 - accuracy: 0.7709 - val_loss: 0.4850 - val_accuracy: 0.7871\n",
      "Epoch 37/50\n",
      "169/177 [===========================>..] - ETA: 0s - loss: 0.5104 - accuracy: 0.7713\n",
      "Epoch 37: val_loss did not improve from 0.45604\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7707 - val_loss: 0.4953 - val_accuracy: 0.7722\n",
      "Epoch 38/50\n",
      "176/177 [============================>.] - ETA: 0s - loss: 0.5075 - accuracy: 0.7734\n",
      "Epoch 38: val_loss did not improve from 0.45604\n",
      "177/177 [==============================] - 0s 3ms/step - loss: 0.5074 - accuracy: 0.7735 - val_loss: 0.4591 - val_accuracy: 0.7999\n",
      "Epoch 39/50\n",
      "173/177 [============================>.] - ETA: 0s - loss: 0.5010 - accuracy: 0.7719\n",
      "Epoch 39: val_loss did not improve from 0.45604\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5010 - accuracy: 0.7721 - val_loss: 0.4871 - val_accuracy: 0.7821\n",
      "Epoch 40/50\n",
      "157/177 [=========================>....] - ETA: 0s - loss: 0.5033 - accuracy: 0.7753\n",
      "Epoch 40: val_loss did not improve from 0.45604\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.4978 - accuracy: 0.7788 - val_loss: 0.4590 - val_accuracy: 0.7885\n",
      "Epoch 41/50\n",
      "157/177 [=========================>....] - ETA: 0s - loss: 0.5084 - accuracy: 0.7721\n",
      "Epoch 41: val_loss did not improve from 0.45604\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5076 - accuracy: 0.7723 - val_loss: 0.4625 - val_accuracy: 0.7878\n",
      "Epoch 42/50\n",
      "173/177 [============================>.] - ETA: 0s - loss: 0.4936 - accuracy: 0.7762\n",
      "Epoch 42: val_loss improved from 0.45604 to 0.45586, saving model to d:\\Customer-Churn-Analysis\\Predictive_Modeling/trained_model\\best_model.h5\n",
      "177/177 [==============================] - 0s 3ms/step - loss: 0.4912 - accuracy: 0.7778 - val_loss: 0.4559 - val_accuracy: 0.7928\n",
      "Epoch 43/50\n",
      "177/177 [==============================] - ETA: 0s - loss: 0.5022 - accuracy: 0.7691\n",
      "Epoch 43: val_loss did not improve from 0.45586\n",
      "177/177 [==============================] - 1s 3ms/step - loss: 0.5022 - accuracy: 0.7691 - val_loss: 0.4895 - val_accuracy: 0.7850\n",
      "Epoch 44/50\n",
      "166/177 [===========================>..] - ETA: 0s - loss: 0.4994 - accuracy: 0.7750\n",
      "Epoch 44: val_loss did not improve from 0.45586\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.4987 - accuracy: 0.7762 - val_loss: 0.4764 - val_accuracy: 0.7793\n",
      "Epoch 45/50\n",
      "172/177 [============================>.] - ETA: 0s - loss: 0.4963 - accuracy: 0.7718\n",
      "Epoch 45: val_loss did not improve from 0.45586\n",
      "177/177 [==============================] - 0s 3ms/step - loss: 0.4960 - accuracy: 0.7717 - val_loss: 0.4648 - val_accuracy: 0.7871\n",
      "Epoch 46/50\n",
      "171/177 [===========================>..] - ETA: 0s - loss: 0.4946 - accuracy: 0.7785\n",
      "Epoch 46: val_loss improved from 0.45586 to 0.45309, saving model to d:\\Customer-Churn-Analysis\\Predictive_Modeling/trained_model\\best_model.h5\n",
      "177/177 [==============================] - 1s 3ms/step - loss: 0.4930 - accuracy: 0.7792 - val_loss: 0.4531 - val_accuracy: 0.7935\n",
      "Epoch 47/50\n",
      "164/177 [==========================>...] - ETA: 0s - loss: 0.4925 - accuracy: 0.7740\n",
      "Epoch 47: val_loss did not improve from 0.45309\n",
      "177/177 [==============================] - 0s 3ms/step - loss: 0.4940 - accuracy: 0.7728 - val_loss: 0.4692 - val_accuracy: 0.7842\n",
      "Epoch 48/50\n",
      "159/177 [=========================>....] - ETA: 0s - loss: 0.5033 - accuracy: 0.7647\n",
      "Epoch 48: val_loss did not improve from 0.45309\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.5049 - accuracy: 0.7652 - val_loss: 0.4583 - val_accuracy: 0.7814\n",
      "Epoch 49/50\n",
      "170/177 [===========================>..] - ETA: 0s - loss: 0.4929 - accuracy: 0.7770\n",
      "Epoch 49: val_loss did not improve from 0.45309\n",
      "177/177 [==============================] - 0s 2ms/step - loss: 0.4944 - accuracy: 0.7774 - val_loss: 0.4544 - val_accuracy: 0.7857\n",
      "Epoch 50/50\n",
      "168/177 [===========================>..] - ETA: 0s - loss: 0.5000 - accuracy: 0.7749\n",
      "Epoch 50: val_loss did not improve from 0.45309\n",
      "177/177 [==============================] - 0s 3ms/step - loss: 0.4987 - accuracy: 0.7756 - val_loss: 0.4539 - val_accuracy: 0.7842\n",
      "Training complete. Model and results saved.\n"
     ]
    }
   ],
   "source": [
    "# Define the model checkpoint to save the best model\n",
    "trained_model_path = os.path.join(base_dir, config['trained_model_path'])\n",
    "checkpoint = ModelCheckpoint(os.path.join(trained_model_path, 'best_model.h5'), \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=50, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    callbacks=[checkpoint])\n",
    "\n",
    "# Save the model architecture\n",
    "ann_architecture_path = os.path.join(base_dir, config['ann_architecture_path'])\n",
    "with open(os.path.join(ann_architecture_path, 'ann_architecture.json'), 'w') as json_file:\n",
    "    model_json = model.to_json()\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save training results\n",
    "results_path = os.path.join(base_dir, config['results_path'])\n",
    "with open(os.path.join(results_path, 'training_results.txt'), 'w') as f:\n",
    "    f.write(str(history.history))\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(results_path, 'training_results.csv'), index=False)\n",
    "\n",
    "print(\"Training complete. Model and results saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "With the trained model, we now make predictions on the testing data. These predictions will be compared to the actual values in the test dataset to evaluate the model's performance.\n",
    "\n",
    "The predictions are saved to a CSV file (`predictions.csv`) for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 1ms/step\n",
      "Predictions saved to d:\\Customer-Churn-Analysis\\Predictive_Modeling/results/predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test)\n",
    "predictions = (predictions > 0.5).astype(int)  # Convert probabilities to binary class predictions\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df = pd.DataFrame({'Actual': y_test, 'Predicted': predictions.flatten()})\n",
    "predictions_path = os.path.join(base_dir, config['predictions_path'])\n",
    "predictions_df.to_csv(predictions_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {predictions_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Tally Function\n",
    "\n",
    "The tally function is used to compare the model's predictions with the actual values in the test dataset. It checks if the predictions match the actual outcomes and saves the results to a CSV file (`tally_results.csv`).\n",
    "\n",
    "This step is crucial for validating the model's accuracy and ensuring that the predictions align with the expected results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tally results saved to d:\\Customer-Churn-Analysis\\Predictive_Modeling/results\\tally_results.csv\n",
      "All predictions match the actual values in test data.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the path to the PredictionTally.py file\n",
    "utils_dir = os.path.abspath('../../utils')\n",
    "prediction_tally_path = os.path.join(utils_dir, 'PredictionTally.py')\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"PredictionTally\", prediction_tally_path)\n",
    "PredictionTally = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(PredictionTally)\n",
    "\n",
    "# Define the base directory\n",
    "base_dir = os.path.dirname(os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "# Now use the function\n",
    "PredictionTally.run_prediction_tally(base_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Obtained\n",
    "\n",
    "After running the predictive modeling process, the following key outputs were obtained:\n",
    "\n",
    "1. **Model Architecture (`ann_architecture.json`)**: This file contains the structure of the ANN model, which includes the configuration of layers and activation functions.\n",
    "2. **Training Results (`training_results.txt`)**: This file provides insights into the training process, including metrics like accuracy and loss over epochs.\n",
    "3. **Best Model (`best_model.h5`)**: The best-performing model based on validation loss during training, which can be used for future predictions.\n",
    "4. **Predictions (`predictions.csv`)**: Contains the predicted values for the test dataset.\n",
    "5. **Tally Results (`tally_results.csv`)**: The results of comparing predictions with actual outcomes, confirming the accuracy of the model.\n",
    "\n",
    "These results validate the effectiveness of the model and provide a basis for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this task, we successfully defined and implemented the architecture of an Artificial Neural Network (ANN) to predict customer churn. The model was trained, validated, and its predictions were tallied against actual outcomes. The process demonstrated the model's capability to learn from historical data and make accurate predictions on new data.\n",
    "\n",
    "Key takeaways include:\n",
    "- The ANN model effectively learned from the provided features to generate predictions.\n",
    "- The use of dropout layers and validation helped in optimizing the model’s generalizability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Predict Customer Churn Based on Critical Attributes:**\n",
    "   - Use the trained ANN model to predict customer churn on the testing dataset.\n",
    "   - Analyze which features (attributes) had the most significant impact on the model's predictions.\n",
    "   - Save the prediction results to a CSV file for further analysis.\n",
    "\n",
    "2. **Evaluate Model Performance and Analyze Predictions:**\n",
    "   - Evaluate the model's performance using metrics such as accuracy, precision, recall, and F1 score.\n",
    "   - Compare the predicted churn values with the actual values in the test dataset.\n",
    "   - Visualize the results using confusion matrices and other relevant plots to gain insights into the model's behavior.\n",
    "\n",
    "3. **Document Ongoing Work:**\n",
    "   - Ensure that all steps, decisions, and results are thoroughly documented in the notebook.\n",
    "   - Summarize findings and prepare a report to present the model's performance and insights gained during the analysis.\n",
    "   - Consider any potential improvements or next iterations to refine the model further.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
