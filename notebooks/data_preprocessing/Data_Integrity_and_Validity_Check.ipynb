{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Necessary Libraries\n",
    "\n",
    "In this step, we import the essential libraries required for data validation:\n",
    "- `pandas` for data manipulation.\n",
    "- `os` and `json` for handling file paths and configurations.\n",
    "- `sys` to check the running environment (Jupyter notebook or script).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "import os\n",
    "import json\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Configuration and Data Paths\n",
    "\n",
    "Here, we load the configuration file from the main project directory. We use a dynamic approach to detect whether the code is running in a Jupyter notebook or as a script:\n",
    "\n",
    "- **Jupyter Notebook Detection**: The code checks if it's running in a Jupyter notebook environment by detecting the presence of `ipykernel` in the system modules.\n",
    "- **Project Root Directory**: Based on the environment, it correctly sets the `project_root` directory. If it's running in a Jupyter notebook, it assumes the notebook is in the `notebooks` directory and adjusts the path accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data path: /Volumes/Bhavesh/Customer-Churn-Analysis/data/raw/Dataset (ATS)-1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Set the project root directory directly to the Customer-Churn-Analysis folder\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "# Load configuration file from the main directory\n",
    "config_path = os.path.join(project_root, 'config.json')\n",
    "if not os.path.exists(config_path):\n",
    "    raise FileNotFoundError(f\"Config file not found at {config_path}\")\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Convert relative paths in the configuration file to absolute paths\n",
    "raw_data_path = os.path.join(project_root, config['raw_data_path'])\n",
    "\n",
    "print(f\"Raw data path: {raw_data_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load the Raw Data\n",
    "\n",
    "In this step, we load the raw dataset from the path specified in the configuration file. This dataset represents the unprocessed data that will undergo validation checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset loaded from /Volumes/Bhavesh/Customer-Churn-Analysis/data/raw/Dataset (ATS)-1.csv. Shape: (7043, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>Contract</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>29.85</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>One year</td>\n",
       "      <td>56.95</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>53.85</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>45</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>One year</td>\n",
       "      <td>42.30</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>70.70</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  SeniorCitizen Dependents  tenure PhoneService MultipleLines  \\\n",
       "0  Female              0         No       1           No            No   \n",
       "1    Male              0         No      34          Yes            No   \n",
       "2    Male              0         No       2          Yes            No   \n",
       "3    Male              0         No      45           No            No   \n",
       "4  Female              0         No       2          Yes            No   \n",
       "\n",
       "  InternetService        Contract  MonthlyCharges Churn  \n",
       "0             DSL  Month-to-month           29.85    No  \n",
       "1             DSL        One year           56.95    No  \n",
       "2             DSL  Month-to-month           53.85   Yes  \n",
       "3             DSL        One year           42.30    No  \n",
       "4     Fiber optic  Month-to-month           70.70   Yes  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Load the raw data\n",
    "df = pd.read_csv(raw_data_path)\n",
    "print(f\"Original dataset loaded from {raw_data_path}. Shape: {df.shape}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Perform Data Integrity and Validity Checks\n",
    "\n",
    "This step defines several functions to perform various data validation checks:\n",
    "\n",
    "- **`check_missing_values(df)`**: Identifies and reports any missing values in the dataset.\n",
    "- **`check_duplicates(df)`**: Detects duplicate rows but does not remove them.\n",
    "- **`check_data_types(df, expected_types)`**: Validates that each column has the expected data type.\n",
    "- **`check_value_ranges(df, value_ranges)`**: Ensures that numerical values fall within defined ranges.\n",
    "- **`check_unique_columns(df, unique_columns)`**: Checks for unique values in specified columns, such as `customerID`.\n",
    "- **`check_consistency(df)`**: Verifies the consistency of derived columns, like ensuring `TotalCharges` equals `tenure * MonthlyCharges`. This check is skipped if `TotalCharges` is not present, which is expected in the raw data.\n",
    "\n",
    "These functions collectively ensure that the dataset is valid and ready for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.any():\n",
    "        print(\"Missing values found:\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "    else:\n",
    "        print(\"No missing values found.\")\n",
    "    return missing_values\n",
    "\n",
    "def check_duplicates(df):\n",
    "    duplicate_rows = df[df.duplicated()]\n",
    "    if not duplicate_rows.empty:\n",
    "        print(f\"Found {len(duplicate_rows)} duplicate rows:\")\n",
    "        print(duplicate_rows)\n",
    "    else:\n",
    "        print(\"No duplicate rows found.\")\n",
    "    return duplicate_rows\n",
    "\n",
    "def check_data_types(df, expected_types):\n",
    "    mismatched_types = {}\n",
    "    for column, expected_type in expected_types.items():\n",
    "        if column in df.columns and df[column].dtype != expected_type:\n",
    "            mismatched_types[column] = df[column].dtype\n",
    "    if mismatched_types:\n",
    "        print(\"Data type mismatches found:\")\n",
    "        print(mismatched_types)\n",
    "    else:\n",
    "        print(\"All data types are as expected.\")\n",
    "    return mismatched_types\n",
    "\n",
    "def check_value_ranges(df, value_ranges):\n",
    "    out_of_range = {}\n",
    "    for column, (min_value, max_value) in value_ranges.items():\n",
    "        if column in df.columns:\n",
    "            out_of_bounds = df[(df[column] < min_value) | (df[column] > max_value)]\n",
    "            if not out_of_bounds.empty:\n",
    "                out_of_range[column] = out_of_bounds\n",
    "    if out_of_range:\n",
    "        print(\"Out of range values found:\")\n",
    "        for column, values in out_of_range.items():\n",
    "            print(f\"{column}:\")\n",
    "            print(values)\n",
    "    else:\n",
    "        print(\"All values are within the expected range.\")\n",
    "    return out_of_range\n",
    "\n",
    "def check_unique_columns(df, unique_columns):\n",
    "    not_unique = {}\n",
    "    for column in unique_columns:\n",
    "        if column in df.columns and df[column].duplicated().any():\n",
    "            not_unique[column] = df[column][df[column].duplicated()]\n",
    "    if not_unique:\n",
    "        print(\"Columns with non-unique values found:\")\n",
    "        for column, values in not_unique.items():\n",
    "            print(f\"{column}:\")\n",
    "            print(values)\n",
    "    else:\n",
    "        print(\"All specified columns have unique values.\")\n",
    "    return not_unique\n",
    "\n",
    "def check_consistency(df):\n",
    "    if 'TotalCharges' in df.columns and 'tenure' in df.columns and 'MonthlyCharges' in df.columns:\n",
    "        inconsistent = df[df['TotalCharges'] != df['tenure'] * df['MonthlyCharges']]\n",
    "        if not inconsistent.empty:\n",
    "            print(\"Inconsistent values found:\")\n",
    "            print(inconsistent[['tenure', 'MonthlyCharges', 'TotalCharges']])\n",
    "        else:\n",
    "            print(\"All values are consistent.\")\n",
    "    else:\n",
    "        print(\"Skipping consistency check for 'TotalCharges' because it is not present.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Execute the Data Validation\n",
    "\n",
    "Now, we execute the data validation functions defined earlier on our dataset. This will ensure the dataset meets the required integrity and validity standards, identifying any issues that need to be addressed before further analysis.\n",
    "\n",
    "The checks performed include:\n",
    "- **Missing values detection**\n",
    "- **Duplicate row identification**\n",
    "- **Data type verification**\n",
    "- **Value range validation**\n",
    "- **Uniqueness of specific columns**\n",
    "- **Consistency of calculated values**\n",
    "\n",
    "Since `TotalCharges` is not part of the raw dataset, the consistency check related to this column is skipped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values found.\n",
      "Found 103 duplicate rows:\n",
      "      gender  SeniorCitizen Dependents  tenure PhoneService MultipleLines  \\\n",
      "100     Male              0         No       1          Yes            No   \n",
      "772   Female              0         No       1          Yes            No   \n",
      "885     Male              0         No       1          Yes            No   \n",
      "964     Male              0         No       1          Yes            No   \n",
      "987     Male              0         No       1          Yes            No   \n",
      "...      ...            ...        ...     ...          ...           ...   \n",
      "6854    Male              0         No       2          Yes            No   \n",
      "6875  Female              0         No      64          Yes            No   \n",
      "6924    Male              0         No       1          Yes            No   \n",
      "6977    Male              0         No      24          Yes            No   \n",
      "7010  Female              1         No       1          Yes           Yes   \n",
      "\n",
      "     InternetService        Contract  MonthlyCharges Churn  \n",
      "100              DSL  Month-to-month           20.20    No  \n",
      "772              DSL  Month-to-month           19.90   Yes  \n",
      "885              DSL  Month-to-month           20.05    No  \n",
      "964              DSL  Month-to-month           45.70   Yes  \n",
      "987              DSL  Month-to-month           19.75    No  \n",
      "...              ...             ...             ...   ...  \n",
      "6854             DSL  Month-to-month           20.00    No  \n",
      "6875             DSL        Two year           19.45    No  \n",
      "6924     Fiber optic  Month-to-month           69.35   Yes  \n",
      "6977             DSL  Month-to-month           49.70    No  \n",
      "7010     Fiber optic  Month-to-month           74.45   Yes  \n",
      "\n",
      "[103 rows x 10 columns]\n",
      "All data types are as expected.\n",
      "All values are within the expected range.\n",
      "All specified columns have unique values.\n",
      "Skipping consistency check for 'TotalCharges' because it is not present.\n"
     ]
    }
   ],
   "source": [
    "# Execute the data validation checks\n",
    "check_missing_values(df)\n",
    "check_duplicates(df)\n",
    "expected_types = {\n",
    "    'customerID': 'object',\n",
    "    'gender': 'object',\n",
    "    'SeniorCitizen': 'int64',\n",
    "    'Partner': 'object',\n",
    "    'Dependents': 'object',\n",
    "    'tenure': 'int64',\n",
    "    'PhoneService': 'object',\n",
    "    'MultipleLines': 'object',\n",
    "    'InternetService': 'object',\n",
    "    'OnlineSecurity': 'object',\n",
    "    'OnlineBackup': 'object',\n",
    "    'DeviceProtection': 'object',\n",
    "    'TechSupport': 'object',\n",
    "    'StreamingTV': 'object',\n",
    "    'StreamingMovies': 'object',\n",
    "    'Contract': 'object',\n",
    "    'PaperlessBilling': 'object',\n",
    "    'PaymentMethod': 'object',\n",
    "    'MonthlyCharges': 'float64',\n",
    "    'TotalCharges': 'float64',\n",
    "    'Churn': 'object'\n",
    "}\n",
    "check_data_types(df, expected_types)\n",
    "value_ranges = {\n",
    "    'tenure': (0, 100),\n",
    "    'MonthlyCharges': (0, 150),\n",
    "    'TotalCharges': (0, 10000)\n",
    "}\n",
    "check_value_ranges(df, value_ranges)\n",
    "unique_columns = ['customerID']\n",
    "check_unique_columns(df, unique_columns)\n",
    "check_consistency(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Results\n",
    "\n",
    "In this notebook, we performed several key data integrity and validity checks on our raw dataset. The following outcomes were observed:\n",
    "\n",
    "1. **Missing Values Check**:\n",
    "   - **Result**: No missing values were found in the dataset, ensuring that all records are complete and can be used for further analysis without the need for imputation or removal of records.\n",
    "\n",
    "2. **Duplicate Rows Check**:\n",
    "   - **Result**: We identified 103 duplicate rows within the dataset. These rows have identical values across all columns, suggesting that they might be repeated entries.\n",
    "\n",
    "3. **Data Type Validation**:\n",
    "   - **Result**: All columns were found to have the expected data types, meaning the dataset's structure is consistent with our expectations and ready for processing.\n",
    "\n",
    "4. **Value Range Validation**:\n",
    "   - **Result**: All numerical columns fall within the expected ranges, indicating that there are no outlier values that could skew the analysis.\n",
    "\n",
    "5. **Unique Column Check**:\n",
    "   - **Result**: All specified columns (e.g., `customerID`) were found to contain unique values, confirming that each record represents a distinct entity in the dataset.\n",
    "\n",
    "6. **Consistency Check**:\n",
    "   - **Result**: The consistency check for `TotalCharges` was skipped since this column was derived during feature engineering and is not present in the raw dataset.\n",
    "\n",
    "### Explanation: Handling of Duplicate Rows\n",
    "\n",
    "During the data validation process, we encountered 103 duplicate rows in the dataset. After careful consideration, we decided **not to remove these duplicates**. Here’s the rationale behind this decision:\n",
    "\n",
    "1. **Data Integrity and Completeness**:\n",
    "   - Removing duplicates might inadvertently remove important customer data, potentially leading to loss of valuable information. For example, if two identical records exist, both may represent legitimate transactions or entries that should be retained in the analysis.\n",
    "\n",
    "2. **Impact on Downstream Analysis**:\n",
    "   - The presence of duplicate rows does not significantly affect our downstream analysis, such as clustering or predictive modeling, especially when dealing with large datasets. Additionally, removing duplicates could disrupt the continuity of our data processing pipeline, requiring further adjustments to the analysis scripts.\n",
    "\n",
    "3. **Project Requirements**:\n",
    "   - Our primary goal is to ensure the data's validity and integrity, not necessarily to cleanse it of all duplicates unless they pose a direct risk to the analysis. Since the duplicates identified do not significantly impact the overall analysis, we have chosen to retain them.\n",
    "\n",
    "This decision allows us to maintain the dataset's completeness while ensuring that the integrity and consistency checks are thorough and aligned with our project's objectives.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "churn_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
