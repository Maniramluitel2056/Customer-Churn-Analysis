{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting\n",
    "This notebook splits the processed dataset into training and testing sets. We will load the processed dataset, split it, and save the resulting training and testing datasets for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Configuration\n",
    "In this step, we import the necessary libraries and load the configuration settings. The configuration file contains paths to various datasets. We convert these paths to absolute paths to ensure the file locations are correctly identified. The paths for processed data, training data, and testing data are printed for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "File `'/d:/Customer-Churn-Analysis/notebooks/data_preprocessing/data_splitting.ipynb'` not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\iambh\\anaconda3\\envs\\churn_analysis\\lib\\site-packages\\IPython\\core\\magics\\execution.py:701\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    700\u001b[0m     fpath \u001b[38;5;241m=\u001b[39m arg_lst[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 701\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43mfile_finder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\iambh\\anaconda3\\envs\\churn_analysis\\lib\\site-packages\\IPython\\utils\\path.py:90\u001b[0m, in \u001b[0;36mget_py_filename\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m py_name\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile `\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m` not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n",
      "\u001b[1;31mOSError\u001b[0m: File `'/d:/Customer-Churn-Analysis/notebooks/data_preprocessing/data_splitting.ipynb'` not found.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load configuration\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-i /d:/Customer-Churn-Analysis/notebooks/data_preprocessing/data_splitting.ipynb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Print the absolute paths for verification\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed data path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessed_data_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\iambh\\anaconda3\\envs\\churn_analysis\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2417\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2415\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2416\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2417\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2419\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[0;32m   2420\u001b[0m \u001b[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[0;32m   2421\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[0;32m   2422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\iambh\\anaconda3\\envs\\churn_analysis\\lib\\site-packages\\IPython\\core\\magics\\execution.py:712\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    710\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\"\u001b[39m,fpath):\n\u001b[0;32m    711\u001b[0m         warn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFor Windows, use double quotes to wrap a filename: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124mun \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmypath\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmyfile.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 712\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    714\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fpath \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmeta_path:\n",
      "\u001b[1;31mException\u001b[0m: File `'/d:/Customer-Churn-Analysis/notebooks/data_preprocessing/data_splitting.ipynb'` not found."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd # type: ignore\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load configuration\n",
    "config_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'config.json')\n",
    "print(f\"Config path: {config_path}\")\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "\n",
    "# Convert relative paths to absolute paths\n",
    "project_root = os.path.dirname(notebook_dir) # type: ignore\n",
    "processed_data_path = os.path.join(project_root, config['processed_data_path'])\n",
    "train_path = os.path.join(project_root, 'data', 'train', 'train_dataset.csv')\n",
    "test_path = os.path.join(project_root, 'data', 'test', 'test_dataset.csv')\n",
    "train_path_prep = os.path.join(project_root, 'Data_Preparation', 'training_sets', 'train_dataset.csv')\n",
    "test_path_prep = os.path.join(project_root, 'Data_Preparation', 'testing_sets', 'test_dataset.csv')\n",
    "\n",
    "# Print the absolute paths for verification\n",
    "print(f\"Processed data path: {processed_data_path}\")\n",
    "print(f\"Train path: {train_path}\")\n",
    "print(f\"Test path: {test_path}\")\n",
    "print(f\"Train path for Data Preparation: {train_path_prep}\")\n",
    "print(f\"Test path for Data Preparation: {test_path_prep}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Processed Data\n",
    "In this step, we define a function `load_data` to load data from a specified CSV file path. We use this function to load the processed dataset. The first few rows of the dataset are displayed to confirm successful loading and to inspect the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Split Data into Training and Testing Sets\n",
    "Here, we define the `split_data` function to separate the dataset into features (`X`) and the target variable (`y`). The data is then split into training and testing sets using an 80-20 split. The training and testing sets are concatenated back into DataFrames and displayed to verify the split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save the Training and Testing Datasets\n",
    "In this step, we save the training and testing datasets to their respective paths specified in the configuration. The paths include locations for both the general data folder and the data preparation folder. Confirmation messages are printed to ensure the datasets are saved correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, we performed the following steps:\n",
    "1. Imported necessary libraries and loaded configuration settings.\n",
    "2. Loaded the processed dataset.\n",
    "3. Split the data into training and testing sets.\n",
    "4. Saved the training and testing datasets.\n",
    "\n",
    "The next steps will involve building and evaluating predictive models using the training and testing datasets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (churn_analysis)",
   "language": "python",
   "name": "churn_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
